[{"C:\\Users\\Conner\\Home\\Projects\\ArtificialThought\\_audio_module\\frontend\\src\\index.tsx":"1","C:\\Users\\Conner\\Home\\Projects\\ArtificialThought\\_audio_module\\frontend\\src\\AudioRecorder.tsx":"2"},{"size":223,"mtime":1707343875540,"results":"3","hashOfConfig":"4"},{"size":10538,"mtime":1707432323714,"results":"5","hashOfConfig":"4"},{"filePath":"6","messages":"7","errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0},"v0gvse",{"filePath":"8","messages":"9","errorCount":1,"fatalErrorCount":1,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"source":"10"},"C:\\Users\\Conner\\Home\\Projects\\ArtificialThought\\_audio_module\\frontend\\src\\index.tsx",[],"C:\\Users\\Conner\\Home\\Projects\\ArtificialThought\\_audio_module\\frontend\\src\\AudioRecorder.tsx",["11"],"import { library } from '@fortawesome/fontawesome-svg-core';\nimport { fas } from \"@fortawesome/free-solid-svg-icons\";\nimport { FontAwesomeIcon } from \"@fortawesome/react-fontawesome\";\nimport React, { ReactNode } from \"react\";\nimport {\n  Streamlit,\n  StreamlitComponentBase,\n  withStreamlitConnection\n} from \"streamlit-component-lib\";\nimport { AssemblyAI, TranscriptService, FileService } from 'assemblyai';\n\nlibrary.add(fas)\n\n// Create an AssemblyAI client with your API key\nconst client = new AssemblyAI({\n  apiKey: process.env.ASSEMBLYAI_API_KEY as string,\n});\n\nconst transcriptService = new TranscriptService({ apiKey: process.env.ASSEMBLYAI_API_KEY as string}, new FileService({ apiKey: process.env.ASSEMBLYAI_API_KEY as string}));\ninterface AudioRecorderState {\n  color: string\n}\n\ninterface AudioData {\n  blob: Blob\n  url: string\n  type: string\n}\n\ninterface AudioRecorderProps {\n  args: Map<string, any>\n  width: number\n  disabled: boolean\n}\n\n\nasync function transcribeAudio(audioBuffer: Buffer): Promise<string> {\n  try {\n    // Transcribe the audio buffer\n    const transcript = await transcriptService.transcribe({ audio: audioBuffer });\n  \n    // Check if the transcription was successful\n    if (transcript.status === 'completed') {\n      return transcript.text as string;\n    } else {\n      throw new Error('Transcription failed');\n    }\n  } catch (error) {\n    console.error(error);\n    return 'An error occurred while transcribing the audio';\n  }\n}\n\nclass AudioRecorder extends StreamlitComponentBase<AudioRecorderState> {\n  public constructor(props: AudioRecorderProps) {\n    super(props)\n    this.state = { color: this.props.args[\"neutral_color\"] }\n  }\n\n  stream: MediaStream | null = null;\n  AudioContext = window.AudioContext || window.webkitAudioContext;\n  type: string = \"audio/wav\";\n  sampleRate: number | null = null;\n  phrase_buffer_count: number | null = null;\n  pause_buffer_count: number | null = null;\n  pause_count: number = 0;\n  stage: string | null = null;\n  volume: any = null;\n  audioInput: any = null;\n  analyser: any = null;\n  recorder: any = null;\n  recording: boolean = false;\n  leftchannel: Float32Array[] = [];\n  rightchannel: Float32Array[] = [];\n  leftBuffer: Float32Array | null = null;\n  rightBuffer: Float32Array | null = null;\n  recordingLength: number = 0;\n  tested: boolean = false;\n\n  //get mic stream\n  getStream = (): Promise<MediaStream> => {\n    return navigator.mediaDevices.getUserMedia({ audio: true, video: false });\n  };\n\n  setupMic = async () => {\n    try {\n      window.stream = this.stream = await this.getStream();\n    } catch (err) {\n      console.log(\"Error: Issue getting mic\", err);\n    }\n\n    this.startRecording();\n  };\n\n  closeMic = () => {\n    this.stream!.getAudioTracks().forEach((track) => {\n      track.stop();\n    });\n    this.audioInput.disconnect(0);\n    this.analyser.disconnect(0);\n    this.recorder.disconnect(0);\n  };\n\n  writeUTFBytes = (view: DataView, offset: number, string: string) => {\n    let lng = string.length;\n    for (let i = 0; i < lng; i++) {\n      view.setUint8(offset + i, string.charCodeAt(i));\n    }\n  };\n\n  mergeBuffers = (channelBuffer: Float32Array[], recordingLength: number) => {\n    let result = new Float32Array(recordingLength);\n    let offset = 0;\n    let lng = channelBuffer.length;\n    for (let i = 0; i < lng; i++) {\n      let buffer = channelBuffer[i];\n      result.set(buffer, offset);\n      offset += buffer.length;\n    }\n    return result;\n  };\n\n  interleave = (leftChannel: Float32Array, rightChannel: Float32Array) => {\n    let length = leftChannel.length + rightChannel.length;\n    let result = new Float32Array(length);\n\n    let inputIndex = 0;\n\n    for (let index = 0; index < length; ) {\n      result[index++] = leftChannel[inputIndex];\n      result[index++] = rightChannel[inputIndex];\n      inputIndex++;\n    }\n    return result;\n  };\n\n  startRecording = () => {\n    let input_sample_rate = this.props.args[\"sample_rate\"];\n    if (input_sample_rate === null) {\n      this.context = new this.AudioContext();\n      this.sampleRate = this.context.sampleRate;\n    } else {\n      this.context = new this.AudioContext(\n        {\"sampleRate\": input_sample_rate}\n      );\n      this.sampleRate = input_sample_rate;\n    }\n    console.log(`Sample rate ${this.sampleRate}Hz`);\n\n    // create buffer states counts\n    let bufferSize = 2048;\n    let seconds_per_buffer = bufferSize / this.sampleRate!;\n    this.pause_buffer_count = Math.ceil(\n      this.props.args[\"pause_threshold\"] / seconds_per_buffer\n    );\n    this.pause_count = 0;\n    this.stage = \"start\";\n\n    // creates a gain node\n    this.volume = this.context.createGain();\n\n    // creates an audio node from teh microphone incoming stream\n    this.audioInput = this.context.createMediaStreamSource(this.stream);\n\n    // Create analyser\n    this.analyser = this.context.createAnalyser();\n\n    // connect audio input to the analyser\n    this.audioInput.connect(this.analyser);\n\n    // connect analyser to the volume control\n    // analyser.connect(volume);\n\n    this.recorder = this.context.createScriptProcessor(bufferSize, 2, 2);\n\n    // we connect the volume control to the processor\n    // volume.connect(recorder);\n\n    this.analyser.connect(this.recorder);\n\n    // finally connect the processor to the output\n    this.recorder.connect(this.context.destination);\n\n    const self = this;  // to reference component from inside the function\n    this.recorder.onaudioprocess = function (e: any) {\n      // Check\n      if (!self.recording) return;\n      // Do something with the data, i.e Convert this to WAV\n      let left = e.inputBuffer.getChannelData(0);\n      let right = e.inputBuffer.getChannelData(1);\n      if (!self.tested) {\n        self.tested = true;\n        // if this reduces to 0 we are not getting any sound\n        if (!left.reduce((a: number, b: number) => a + b)) {\n          console.log(\"Error: There seems to be an issue with your Mic\");\n          // clean up;\n          self.stop();\n          self.stream!.getTracks().forEach(function (track: any) {\n            track.stop();\n          });\n          self.context.close();\n        }\n      }\n      // Check energy level\n      let energy = Math.sqrt(\n        left.map((x: number) => x * x).reduce((a: number, b: number) => a + b) / left.length\n      );\n      if (self.stage === \"start\" && energy > self.props.args[\"start_threshold\"]) {\n        self.stage = \"speaking\";\n      } else if (self.stage === \"speaking\") {\n        if (energy > self.props.args[\"end_threshold\"]) {\n          self.pause_count = 0;\n        } else {\n          self.pause_count += 1;\n          if (self.pause_count > self.pause_buffer_count!) {\n            self.stop();\n          }\n        }\n      }\n      // let radius = 33.0 + Math.sqrt(1000.0 * energy);\n      // this.props.setRadius(radius.toString());\n\n      // we clone the samples\n      self.leftchannel.push(new Float32Array(left));\n      self.rightchannel.push(new Float32Array(right));\n      self.recordingLength += bufferSize;\n    };\n    // this.visualize();\n  };\n\n  start = async () => {\n    console.log(\"Starting recording, previous state\", this.recording);\n    this.recording = true;\n    this.setState({\n      color: this.props.args[\"recording_color\"]\n    })\n    await this.setupMic();\n    // reset the buffers for the new recording\n    this.leftchannel.length = this.rightchannel.length = 0;\n    this.recordingLength = 0;    \n    console.log(\"Started recording, new state\", this.recording);\n  };\n\n  stop = async () => {\n    console.log(\"Stopping recording, previous state\", this.recording);\n    this.recording = false;\n    this.setState({\n      color: this.props.args[\"neutral_color\"]\n    })\n    this.closeMic();\n    console.log(this.recordingLength);\n\n    // we flat the left and right channels down\n    this.leftBuffer = this.mergeBuffers(this.leftchannel, this.recordingLength);\n    this.rightBuffer = this.mergeBuffers(\n      this.rightchannel,\n      this.recordingLength\n    );\n    // we interleave both channels together\n    let interleaved = this.interleave(this.leftBuffer, this.rightBuffer);\n\n    ///////////// WAV Encode /////////////////\n    // from http://typedarray.org/from-microphone-to-wav-with-getusermedia-and-web-audio/\n    //\n\n    // we create our wav file\n    let buffer = new ArrayBuffer(44 + interleaved.length * 2);\n    let view = new DataView(buffer);\n\n    // RIFF chunk descriptor\n    this.writeUTFBytes(view, 0, \"RIFF\");\n    view.setUint32(4, 44 + interleaved.length * 2, true);\n    this.writeUTFBytes(view, 8, \"WAVE\");\n    // FMT sub-chunk\n    this.writeUTFBytes(view, 12, \"fmt \");\n    view.setUint32(16, 16, true);\n    view.setUint16(20, 1, true);\n    // stereo (2 channels)\n    view.setUint16(22, 2, true);\n    view.setUint32(24, this.sampleRate!, true);\n    view.setUint32(28, this.sampleRate! * 4, true);\n    view.setUint16(32, 4, true);\n    view.setUint16(34, 16, true);\n    // data sub-chunk\n    this.writeUTFBytes(view, 36, \"data\");\n    view.setUint32(40, interleaved.length * 2, true);\n\n    // write the PCM samples\n    let lng = interleaved.length;\n    let index = 44;\n    let volume = 1;\n    for (let i = 0; i < lng; i++) {\n      view.setInt16(index, interleaved[i] * (0x7fff * volume), true);\n      index += 2;\n    }\n\n    // our final binary blob\n    const blob = new Blob([view], { type: this.type });\n    const audioUrl = URL.createObjectURL(blob);\n\n\n    await this.onStop({\n      blob: blob,\n      url: audioUrl,\n      type: this.type,\n    });\n    console.log(\"Stopped recording, new state\", this.recording);\n  };\n\n  public render = (): ReactNode => {\n    const { theme } = this.props\n    const text = this.props.args[\"text\"]\n\n    if (theme) {\n      // Maintain compatibility with older versions of Streamlit that don't send\n      // a theme object.\n    }\n\n    return (\n      <span>\n        {text} &nbsp;\n        <FontAwesomeIcon\n        // @ts-ignore\n        icon={this.props.args[\"icon_name\"]}\n        onClick={this.onClicked}\n        style={{color:this.state.color}}\n        size={this.props.args[\"icon_size\"]}\n        />\n      </span>\n    )\n  }\n\n  private onClicked = async () => {\n    console.log(\"Clicked\")\n    if (!this.recording){\n      await this.start()\n    } else {\n      await this.stop()\n    }\n\n  }\n    \n  private onStop = async (data: AudioData) => {\n    var buffer = await data.blob.arrayBuffer();\n  \n    // Transcribe audio to text\n    var transcript = await transcribeAudio(new Buffer(buffer));\n    Streamlit.setComponentValue(transcript);\n  }\n\n} \n  \n\nexport default withStreamlitConnection(AudioRecorder)",{"ruleId":null,"fatal":true,"severity":2,"message":"12","line":16,"column":42},"Parsing error: Unexpected token, expected \",\"\n\n\u001b[0m \u001b[90m 14 |\u001b[39m \u001b[90m// Create an AssemblyAI client with your API key\u001b[39m\u001b[0m\n\u001b[0m \u001b[90m 15 |\u001b[39m \u001b[36mconst\u001b[39m client \u001b[33m=\u001b[39m \u001b[36mnew\u001b[39m \u001b[33mAssemblyAI\u001b[39m({\u001b[0m\n\u001b[0m\u001b[31m\u001b[1m>\u001b[22m\u001b[39m\u001b[90m 16 |\u001b[39m   apiKey\u001b[33m:\u001b[39m process\u001b[33m.\u001b[39menv\u001b[33m.\u001b[39m\u001b[33mASSEMBLYAI_API_KEY\u001b[39m \u001b[36mas\u001b[39m string\u001b[33m,\u001b[39m\u001b[0m\n\u001b[0m \u001b[90m    |\u001b[39m                                          \u001b[31m\u001b[1m^\u001b[22m\u001b[39m\u001b[0m\n\u001b[0m \u001b[90m 17 |\u001b[39m })\u001b[33m;\u001b[39m\u001b[0m\n\u001b[0m \u001b[90m 18 |\u001b[39m\u001b[0m\n\u001b[0m \u001b[90m 19 |\u001b[39m \u001b[36mconst\u001b[39m transcriptService \u001b[33m=\u001b[39m \u001b[36mnew\u001b[39m \u001b[33mTranscriptService\u001b[39m({ apiKey\u001b[33m:\u001b[39m process\u001b[33m.\u001b[39menv\u001b[33m.\u001b[39m\u001b[33mASSEMBLYAI_API_KEY\u001b[39m \u001b[36mas\u001b[39m string}\u001b[33m,\u001b[39m \u001b[36mnew\u001b[39m \u001b[33mFileService\u001b[39m({ apiKey\u001b[33m:\u001b[39m process\u001b[33m.\u001b[39menv\u001b[33m.\u001b[39m\u001b[33mASSEMBLYAI_API_KEY\u001b[39m \u001b[36mas\u001b[39m string}))\u001b[33m;\u001b[39m\u001b[0m"]