{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Conner\\anaconda3\\envs\\athought\\Lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.tools import tool\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Conner\\AppData\\Local\\Temp\\ipykernel_21476\\490271908.py:2: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(\"Your test query here\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='177f3621-f5ce-4e92-8631-5d6b5b112d65', metadata={'file_name': 'AI_Apps_with_LangChain_v2_MEAP.pdf', 'page': 85.0, 'source': '_local_vault\\\\unprocessed\\\\AI_Apps_with_LangChain_v2_MEAP.pdf'}, page_content='Figure 4.4 Automated research summarization engine: Ask a question, and the engine performs a web search,\\nreturns URLs, scrapes and summarizes web pages, and compiles a research report for you.\\n\\xa0\\xa0\\xa0\\xa0 The architecture diagram in ﬁgure 4.4 shows room for improvement: instead of directly sending the\\noriginal research request to the web search engine, you can take advantage of the LLM to formulate\\nmultiple web search queries. This technique, known as \"query rewriting\" or \"multiple query\\ngeneration,\" draws inspiration from a similar method used to enhance RAG searches, which we\\'ll\\ndelve into in subsequent chapters.\\n\\xa0\\xa0\\xa0\\xa0For instance, rather than querying the search engine \"How many titles did Michael Jordan win?\",\\nyou can prompt ChatGPT as follows:\\n\\xa0\\xa0\\xa0\\xa0\\nYou\\'ll receive these queries:\\n\\xa0\\xa0\\xa0\\xa0\\nConsequently, you\\'ll conduct research by performing these three web searches and gathering\\nresults from each. This updated workﬂow is depicted in the diagram in ﬁgure 4.5.\\n\\xa0\\xa0\\xa0\\xa0Generate three web search queries to research the following topic, aiming for a  \\ncomprehensive report: \"How many titles did Michael Jordan win?\"\\n\\xa0\\xa0\\xa0\\xa0\\n1. \"Michael Jordan NBA championships total\"\\n2. \"List of NBA titles won by Michael Jordan\"\\n3. \"Michael Jordan basketball career championships count\"\\n\\xa0\\xa0\\xa0\\xa0\\n© Manning Publications Co. To comment go to liveBook\\n82\\nLicensed to Conner Dimoush <cdimoush@gmail.com>'), Document(id='8233441a-d73d-409e-b084-de34fd0fa844', metadata={'file_name': 'AI_Apps_with_LangChain_v2_MEAP.pdf', 'page': 136.0, 'source': '_local_vault\\\\unprocessed\\\\AI_Apps_with_LangChain_v2_MEAP.pdf'}, page_content='Let\\'s test it with the original question:\\n\\xa0\\xa0\\xa0\\xa0\\nWe get:\\n\\xa0\\xa0\\xa0\\xa0\\nThe synthetized response is comprehensive as it answers all the questions we asked.\\n\\xa0\\xa0\\xa0\\xa0You should be proud of what you have achieved so far! You’ve implemented a basic chatbot that\\ncan answer questions based on text imported into the vector database and provide additional\\ninformation if needed. It won\\'t return information not in the vector database, so it won\\'t hallucinate\\nor make up answers.\\n\\xa0\\xa0\\xa0\\xa0The key takeaway is that you now understand the internals of a Q&A LLM-based system, the\\ncomponents of the RAG design pattern, and its workﬂow. This knowledge will help you when using\\nframeworks like LangChain, LlamaIndex, and Semantic Kernel, which might hide their\\nimplementation details. You’ll be better equipped to troubleshoot problems and understand what’s\\ngoing on behind the scenes.\\n\\xa0\\xa0\\xa0\\xa0Before re-implementing RAG with LangChain, let’s recap the RAG terminology you’ve learned so\\nfar.\\n\\xa0\\xa0\\xa0\\xa0#A Retrieve content from vector store\\n#B Create LLM prompt\\n#C Execute LLM promptdef my_chatbot(question):\\n   results_text = query_vector_database(question)                #A\\n   prompt_input = prompt_template(question, results_text)        #B\\n   prompt_output = execute_llm_prompt(prompt_input)              #C\\n   return prompt_output\\n\\xa0\\xa0\\xa0\\xa0\\nquestion = \"Let me know how many temples there are in Paestum, who constructed them, and  \\nwhat architectural style they are.\"\\nresult = my_chatbot(question)\\nprint(result)\\n\\xa0\\xa0\\xa0\\xa0\\nChatCompletion(id=\\'chatcmpl-9nCmZjXJHXVEZLjNpQdAlrnm3K691\\', choices=\\n[Choice(finish_reason=\\'stop\\', index=0, logprobs=None,  \\nmessage=ChatCompletionMessage(content=\\'There are three Doric temples in Paestum. They  \\nwere constructed by the ancient Greeks, with the Temple of Athena and the Temple of Hera  \\nI dating from the 6th century BC, and the Temple of Hera II built around 460 BC. The  \\narchitectural style of these temples is Doric.\\', role=\\'assistant\\', function_call=None,  \\ntool_calls=None))], created=1721514235, model=\\'gpt-4o-mini-2024-07-18\\', \\nobject=\\'chat.completion\\', service_tier=None, system_fingerprint=\\'fp_8b761cb050\\',  \\nusage=CompletionUsage(completion_tokens=61, prompt_tokens=396, total_tokens=457))\\n\\xa0\\xa0\\xa0\\xa0\\n© Manning Publications Co. To comment go to liveBook\\n133\\nLicensed to Conner Dimoush <cdimoush@gmail.com>'), Document(id='040128dd-c02f-4db6-a98a-25885460e9b4', metadata={'file_name': 'Generative_AI_in_Action.pdf', 'page': 261.0, 'source': '_local_vault\\\\unprocessed\\\\Generative_AI_in_Action.pdf'}, page_content='236 CHAPTER  8Chatting with your data\\n        print(\"No results found\")\\n    return message + questiondef ask_gpt(query : str, max_token = 4096, debug_message = False) -> str:\\n    message = get_search_results(                             query,\\n        max_token,\\n        debug_message=debug_message)    \\n    messages = [                                  \\n        {\"role\":          \"system\", \\n         \"content\": \"You answer questions in summary from the [CA]\\n                     blog posts.\"},        {\"role\":\\n          \"user\",\\n            \"content\": message},]   \\n    response = openai.ChatCompletion.create(          \\n        model=\"gpt-3.5-turbo-16k\",        messages=messages,        temperature=0.7,\\n        max_tokens=2000,\\n        top_p=0.95    )\\n    response_message = response[\"choices\"][0][\"message\"][\"content\"]\\n    return response_message\\nif __name__ == \"__main__\":\\n    # Enter a query    while True:\\n        query = input(\"Please enter your query: \")\\n        print(ask_gpt(query, max_token=15000, debug_message=False))        print(\"==\"*20)\\nWe can see all this coming together when we  run it and chat with the blog. It under-\\nstands the query, creates embeddings, uses the vector database and the associated vec-\\ntor indexes to retrieve the top five matching  results, adds that to the prompt, and uses\\nthe LLM to generate the response (figure 8.7).\\n In the example we have seen thus far, we are responsible for everything—from set-\\nting up the Docker containers to deploying Redis and ingesting the data. This is notenough for enterprises to go into producti on. More system engineering is required,\\nsuch as setting up various clusters of mach ines, scaling them up or down as needed,\\nmanaging Redis, security requ irements, overall operations, and so forth. This takes a\\nsignificant amount of time, effort, cost, an d skills that not every organization might\\nhave. Another option is to use Azure OpenAI , which can do much of this out of the\\nbox and allows organizations a quicker time  to market, potentially at a lower cost.\\nLet’s see how Azure OpenAI can achieve the same result but much faster.Runs a vector search \\nto get embeddings\\nSets up the chat \\ncompletion calls\\nCalls \\nthe LLM \\nLicensed to Conner Dimoush <cdimoush@gmail.com>')]\n"
     ]
    }
   ],
   "source": [
    "retriever = PineconeVectorStore(index_name='vector-vault-1', embedding=OpenAIEmbeddings()).as_retriever(search_kwargs={\"k\": 3})\n",
    "docs = retriever.get_relevant_documents(\"Your test query here\")\n",
    "print(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_name': 'AI_Apps_with_LangChain_v2_MEAP.pdf',\n",
       " 'page': 85.0,\n",
       " 'source': '_local_vault\\\\unprocessed\\\\AI_Apps_with_LangChain_v2_MEAP.pdf'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "athought",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
